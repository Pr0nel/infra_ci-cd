name: CI/CD Pipeline for Multi-Cloud Deployment

on:
  push:
    branches:
      - main
  workflow_dispatch:  # Permite ejecutar manualmente el pipeline desde GitHub

env:
  CLOUD_PROVIDER: ${{ secrets.CLOUD_PROVIDER }}  # AWS, Azure o GCP
  TF_VAR_region: ${{ secrets.TF_VAR_REGION }}    # RegiÃ³n especÃ­fica de la nube
  TF_VAR_bucket_name: ${{ secrets.TF_VAR_BUCKET_NAME }}  # Nombre del bucket u otros recursos
  TF_BACKEND_BUCKET: ${{ secrets.TF_BACKEND_BUCKET }}  # Nombre del bucket donde Terraform guarda su estado (permanente)
  PROJECT_REPO_URL: "https://github.com/Pr0nel/databricks-poc"
  PROJECT_VERSION: "v1.0.0"  # VersiÃ³n especÃ­fica del proyecto
  DATA_PLATFORM: ""  # Se detectarÃ¡ automÃ¡ticamente
  ENABLE_ROLLBACK: "true"

jobs:
  validate-cloud-provider:
    runs-on: ubuntu-latest
    steps:
      # Paso 1: Validar proveedor de nube
      - name: Validate Cloud Provider & Required Secrets
        run: |
          set -euo pipefail
          if [[ ! "${{ env.CLOUD_PROVIDER }}" =~ ^(aws|azure|gcp)$ ]]; then
            echo "Error: CLOUD_PROVIDER debe ser 'aws', 'azure' o 'gcp'"
            exit 1
          fi
          echo "Proveedor de nube vÃ¡lido: ${{ env.CLOUD_PROVIDER }}"
          case "${{ env.CLOUD_PROVIDER }}" in
            aws)
              [[ -n "${{ secrets.AWS_ACCESS_KEY_ID }}" ]] || { echo "Falta AWS_ACCESS_KEY_ID"; exit 1; }
              [[ -n "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]] || { echo "Falta AWS_SECRET_ACCESS_KEY"; exit 1; }
              [[ -n "${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}" ]] || { echo "Falta TF_BACKEND_DYNAMODB_TABLE"; exit 1; }
              ;;
            azure)
              [[ -n "${{ secrets.AZURE_CREDENTIALS }}" ]] || { echo "Falta AZURE_CREDENTIALS"; exit 1; }
              [[ -n "${{ secrets.TF_BACKEND_STORAGE_ACCOUNT }}" ]] || { echo "Falta TF_BACKEND_STORAGE_ACCOUNT"; exit 1; }
              [[ -n "${{ secrets.TF_BACKEND_RESOURCE_GROUP }}" ]] || { echo "Falta TF_BACKEND_RESOURCE_GROUP"; exit 1; }
              [[ -n "${{ secrets.TF_BACKEND_CONTAINER_NAME }}" ]] || { echo "Falta TF_BACKEND_CONTAINER_NAME"; exit 1; }
              ;;
            gcp)
              [[ -n "${{ secrets.GCP_PROJECT_ID }}" ]] || { echo "Falta GCP_PROJECT_ID"; exit 1; }
              [[ -n "${{ secrets.GCP_CREDENTIALS_JSON }}" ]] || { echo "Falta GCP_CREDENTIALS_JSON"; exit 1; }
              ;;
          esac
          echo "Todos los secretos requeridos estÃ¡n presentes"

  setup-cloud-provider:
    needs: validate-cloud-provider
    runs-on: ubuntu-latest
    steps:
      # Paso 2: Definir la nube a usar

      # ConfiguraciÃ³n especÃ­fica para AWS
      - name: Configure AWS CLI
        if: env.CLOUD_PROVIDER == 'aws'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.TF_VAR_region }}      

      - name: Validate AWS Credentials
        if: env.CLOUD_PROVIDER == 'aws'
        run: |
          set -euo pipefail
          echo "Validando credenciales de AWS..."
          aws sts get-caller-identity
          echo "Credenciales de AWS validadas"

      - name: Setup AWS Backend Bucket
        if: env.CLOUD_PROVIDER == 'aws'
        run: |
          set -euo pipefail
          echo "Configurando bucket de backend para Terraform..."
          
          BUCKET_NAME="${{ env.TF_BACKEND_BUCKET }}"
          REGION="${{ env.TF_VAR_region }}"
          DYNAMODB_TABLE="${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}"
          
          if aws s3 ls s3://${BUCKET_NAME} &>/dev/null; then
            echo "Bucket ${BUCKET_NAME} ya existe"
          else
            echo "Creando bucket ${BUCKET_NAME}..."
            aws s3 mb s3://${BUCKET_NAME} --region ${REGION}
            aws s3api put-bucket-versioning \
              --bucket ${BUCKET_NAME} \
              --versioning-configuration Status=Enabled
            aws s3api put-bucket-encryption \
              --bucket ${BUCKET_NAME} \
              --server-side-encryption-configuration '{
                "Rules": [{
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }]
              }'
            echo "Bucket S3 creado exitosamente"
          fi
          if aws dynamodb describe-table --table-name ${DYNAMODB_TABLE} --region ${REGION} &>/dev/null; then
            echo "Tabla DynamoDB ${DYNAMODB_TABLE} ya existe"
          else
            echo "Creando tabla DynamoDB ${DYNAMODB_TABLE}..."
            aws dynamodb create-table \
              --table-name ${DYNAMODB_TABLE} \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region ${REGION}
            echo "Esperando que la tabla estÃ© disponible..."
            aws dynamodb wait table-exists --table-name ${DYNAMODB_TABLE} --region ${REGION}
            echo "Tabla DynamoDB creada exitosamente"
          fi

      # ConfiguraciÃ³n especÃ­fica para Azure
      - name: Azure Login
        if: env.CLOUD_PROVIDER == 'azure'
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Validate Azure Credentials
        if: env.CLOUD_PROVIDER == 'azure'
        run: |
          set -euo pipefail
          echo "Validando credenciales de Azure..."
          az account show
          echo "Credenciales de Azure validadas"

      - name: Setup Azure Backend Storage
        if: env.CLOUD_PROVIDER == 'azure'
        run: |
          set -euo pipefail
          echo "Configurando storage de backend para Terraform..."
          
          RESOURCE_GROUP="${{ secrets.TF_BACKEND_RESOURCE_GROUP }}"
          STORAGE_ACCOUNT="${{ secrets.TF_BACKEND_STORAGE_ACCOUNT }}"
          CONTAINER_NAME="${{ secrets.TF_BACKEND_CONTAINER_NAME }}"
          LOCATION="${{ env.TF_VAR_region }}"
          
          if az group show --name ${RESOURCE_GROUP} &>/dev/null; then
            echo "Resource Group ${RESOURCE_GROUP} ya existe"
          else
            echo "Creando Resource Group ${RESOURCE_GROUP}..."
            az group create --name ${RESOURCE_GROUP} --location ${LOCATION}
            echo "Resource Group creado exitosamente"
          fi
          if az storage account show --name ${STORAGE_ACCOUNT} --resource-group ${RESOURCE_GROUP} &>/dev/null; then
            echo "Storage Account ${STORAGE_ACCOUNT} ya existe"
          else
            echo "Creando Storage Account ${STORAGE_ACCOUNT}..."
            az storage account create \
              --name ${STORAGE_ACCOUNT} \
              --resource-group ${RESOURCE_GROUP} \
              --location ${LOCATION} \
              --sku Standard_LRS \
              --encryption-services blob
            echo "Storage Account creado exitosamente"
          fi
          ACCOUNT_KEY=$(az storage account keys list \
            --resource-group ${RESOURCE_GROUP} \
            --account-name ${STORAGE_ACCOUNT} \
            --query '[0].value' -o tsv)
          
          if az storage container show \
            --name ${CONTAINER_NAME} \
            --account-name ${STORAGE_ACCOUNT} \
            --account-key ${ACCOUNT_KEY} &>/dev/null; then
            echo "Container ${CONTAINER_NAME} ya existe"
          else
            echo "Creando Container ${CONTAINER_NAME}..."
            az storage container create \
              --name ${CONTAINER_NAME} \
              --account-name ${STORAGE_ACCOUNT} \
              --account-key ${ACCOUNT_KEY}
            echo "Container creado exitosamente"
          fi

      # ConfiguraciÃ³n especÃ­fica para GCP
      - name: Authenticate to Google Cloud
        if: env.CLOUD_PROVIDER == 'gcp'
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS_JSON }}
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Setup Google Cloud CLI
        if: env.CLOUD_PROVIDER == 'gcp'
        uses: google-github-actions/setup-gcloud@v2

      - name: Setup GCP Backend Bucket
        if: env.CLOUD_PROVIDER == 'gcp'
        run: |
          set -euo pipefail
          BUCKET_NAME="${{ env.TF_BACKEND_BUCKET }}"
          LOCATION="${{ env.TF_VAR_region }}"
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          if gsutil ls "gs://${BUCKET_NAME}" &>/dev/null; then
            echo "Bucket gs://${BUCKET_NAME} ya existe"
          else
            echo "Creando bucket gs://${BUCKET_NAME}..."
            gsutil mb -p "${PROJECT_ID}" -l "${LOCATION}" "gs://${BUCKET_NAME}"
            gsutil versioning set on "gs://${BUCKET_NAME}"
            echo "Bucket creado exitosamente"
          fi

      - name: Verify GCP Authentication
        if: env.CLOUD_PROVIDER == 'gcp'
        run: |
          set -euo pipefail
          echo "Verificando autenticaciÃ³n con GCP..."
          echo "Autenticado como: $(gcloud config get-value account)"
          echo "Proyecto: $(gcloud config get-value project)"
          echo "RegiÃ³n configurada: ${{ env.TF_VAR_region }}"

  deploy-infrastructure:
    needs: setup-cloud-provider
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      # Paso 3: Checkout del cÃ³digo de infraestructura
      - name: Checkout Infrastructure Code
        uses: actions/checkout@v4

      # Paso 4: Configurar Terraform
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      # Paso 5: Instalar herramientas comunes
      - name: Install Common Tools
        run: |
          set -euo pipefail
          sudo apt-get update && sudo apt-get install -y jq curl unzip
          echo "Herramientas instaladas correctamente"

      # Paso 6: Validar estructura de directorios de Terraform
      - name: Validate Terraform Structure
        run: |
          set -euo pipefail
          if [[ ! -d "terraform/${{ env.CLOUD_PROVIDER }}" ]]; then
            echo "Error: No existe el directorio terraform/${{ env.CLOUD_PROVIDER }}"
            exit 1
          fi
          echo "Estructura de Terraform validada para ${{ env.CLOUD_PROVIDER }}"

      # Paso 7: Inicializar Terraform segÃºn el proveedor de nube
      - name: Initialize Terraform
        run: |
          set -euo pipefail
          cd terraform/${{ env.CLOUD_PROVIDER }}
          echo "Inicializando Terraform para ${{ env.CLOUD_PROVIDER }}..."
          case "${{ env.CLOUD_PROVIDER }}" in
            gcp)
              terraform init \
                -backend-config="bucket=${{ env.TF_BACKEND_BUCKET }}" \
                -input=false \
                -upgrade
              ;;
            aws)
              terraform init \
                -backend-config="bucket=${{ env.TF_BACKEND_BUCKET }}" \
                -backend-config="region=${{ env.TF_VAR_region }}" \
                -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}" \
                -input=false \
                -upgrade
              ;;
            azure)
              terraform init \
                -backend-config="storage_account_name=${{ secrets.TF_BACKEND_STORAGE_ACCOUNT }}" \
                -backend-config="resource_group_name=${{ secrets.TF_BACKEND_RESOURCE_GROUP }}" \
                -input=false \
                -upgrade
              ;;
            *)
              echo "Error: Proveedor de nube no soportado: ${{ env.CLOUD_PROVIDER }}"
              exit 1
              ;;
          esac
          echo "Terraform inicializado correctamente con backend remoto"

      # Paso 8: Validar configuraciÃ³n de Terraform
      - name: Validate Terraform Configuration
        run: |
          set -euo pipefail
          cd terraform/${{ env.CLOUD_PROVIDER }}
          terraform validate
          echo "ConfiguraciÃ³n de Terraform validada"

      # Paso 9: Planificar cambios en la infraestructura
      - name: Terraform Plan
        run: |
          set -euo pipefail
          cd terraform/${{ env.CLOUD_PROVIDER }}
          terraform plan -input=false -out=tfplan
          echo "Plan de Terraform generado"

      # Paso 10: Aplicar cambios en la infraestructura
      - name: Terraform Apply
        run: |
          set -euo pipefail
          cd terraform/${{ env.CLOUD_PROVIDER }}
          terraform apply -input=false -auto-approve tfplan
          echo "Infraestructura desplegada exitosamente"

      # Paso 11: Mostrar salidas de Terraform
      - name: Terraform Outputs
        run: |
          set -euo pipefail
          cd terraform/${{ env.CLOUD_PROVIDER }}
          terraform output -json > ../../terraform_outputs.json
          terraform output
          echo "Outputs de Terraform guardados"

      - name: Upload Terraform Outputs as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs
          path: terraform_outputs.json
          retention-days: 7

      - name: Extract Storage Resources
        run: |
          set -euo pipefail
          cd terraform/${{ env.CLOUD_PROVIDER }}
          echo "Extrayendo recursos de almacenamiento..."
          case "${{ env.CLOUD_PROVIDER }}" in
            aws)
              BUCKET_NAME=$(terraform output -raw s3_bucket_name 2>/dev/null || echo "")
              BUCKET_ARN=$(terraform output -raw s3_bucket_arn 2>/dev/null || echo "")
              echo "S3_BUCKET=${BUCKET_NAME}" >> $GITHUB_ENV
              echo "S3_BUCKET_ARN=${BUCKET_ARN}" >> $GITHUB_ENV
              ;;
            azure)
              STORAGE_ACCOUNT=$(terraform output -raw storage_account_name 2>/dev/null || echo "")
              CONTAINER=$(terraform output -raw container_name 2>/dev/null || echo "")
              echo "AZURE_STORAGE_ACCOUNT=${STORAGE_ACCOUNT}" >> $GITHUB_ENV
              echo "AZURE_CONTAINER=${CONTAINER}" >> $GITHUB_ENV
              ;;
            gcp)
              BUCKET_NAME=$(terraform output -raw gcs_bucket_name 2>/dev/null || echo "")
              echo "GCS_BUCKET=${BUCKET_NAME}" >> $GITHUB_ENV
              ;;
          esac
          echo "Recursos de almacenamiento extraÃ­dos"

  clone-project:
    needs: deploy-infrastructure
    runs-on: ubuntu-latest
    steps:
      # Paso 12: Clonar el repositorio del proyecto
      - name: Clone Project Repository & Checkout Specific Version of Project
        run: |
          set -euo pipefail
          git clone ${{ env.PROJECT_REPO_URL }} project-code
          cd project-code
          git checkout ${{ env.PROJECT_VERSION }}
          echo "CÃ³digo del proyecto clonado y cambiado a versiÃ³n ${{ env.PROJECT_VERSION }}"

      - name: Upload Project Code as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: project-code
          path: project-code/
          retention-days: 7

  detect-data-platform:
    needs: clone-project
    runs-on: ubuntu-latest
    outputs:
      platform: ${{ steps.detect.outputs.platform }}
      platform_version: ${{ steps.detect.outputs.version }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download project artifacts
        uses: actions/download-artifact@v4
        with:
          name: project-code
          path: project-code
        continue-on-error: true

      - name: Detect Data Platform
        id: detect
        run: |
          set -euo pipefail
          REPO_URL="${{ env.PROJECT_REPO_URL }}"
          if [[ "${REPO_URL}" == *"databricks"* ]] || [[ -f "project-code/databricks.yml" ]]; then
            echo "platform=databricks" >> $GITHUB_OUTPUT
            echo "version=latest" >> $GITHUB_OUTPUT
            echo "Plataforma detectada: Databricks"
          elif [[ "${REPO_URL}" == *"snowflake"* ]] || [[ -f "project-code/snowflake.yml" ]]; then
            echo "platform=snowflake" >> $GITHUB_OUTPUT
            echo "version=latest" >> $GITHUB_OUTPUT
            echo "Plataforma detectada: Snowflake"
          else
            echo "Error: No se pudo detectar la plataforma de datos"
            echo "Verifica que PROJECT_REPO_URL contenga 'databricks' o 'snowflake'"
            exit 1
          fi

      - name: Validate Platform Secrets
        run: |
          set -euo pipefail
          PLATFORM="${{ steps.detect.outputs.platform }}"
          case "${PLATFORM}" in
            databricks)
              [[ -n "${{ secrets.DATABRICKS_HOST }}" ]] || { echo "Falta DATABRICKS_HOST"; exit 1; }
              [[ -n "${{ secrets.DATABRICKS_TOKEN }}" ]] || { echo "Falta DATABRICKS_TOKEN"; exit 1; }
              ;;
            snowflake)
              [[ -n "${{ secrets.SNOWFLAKE_ACCOUNT }}" ]] || { echo "Falta SNOWFLAKE_ACCOUNT"; exit 1; }
              [[ -n "${{ secrets.SNOWFLAKE_USER }}" ]] || { echo "Falta SNOWFLAKE_USER"; exit 1; }
              [[ -n "${{ secrets.SNOWFLAKE_PASSWORD }}" ]] || { echo "Falta SNOWFLAKE_PASSWORD"; exit 1; }
              [[ -n "${{ secrets.SNOWFLAKE_WAREHOUSE }}" ]] || { echo "Falta SNOWFLAKE_WAREHOUSE"; exit 1; }
              ;;
          esac
          echo "ValidaciÃ³n de secretos completada"

  deploy-script-cloud-provider:
    needs: detect-data-platform
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      # Paso 13: Validar y ejecuta scripts de despliegue segÃºn el proveedor de nube
      - name: Run Deployment Script
        env:
          DATA_PLATFORM: ${{ needs.detect-data-platform.outputs.platform }}
        run: |
          set -euo pipefail
          SCRIPT_PATH="scripts/deploy_${{ env.CLOUD_PROVIDER }}.sh"
          if [[ ! -f "${SCRIPT_PATH}" ]]; then
            echo "Error: No existe el script de despliegue ${SCRIPT_PATH}"
            exit 1
          fi
          echo "Verificando la sintaxis del script..."
          bash -n "${SCRIPT_PATH}"
          if [[ ! -x "${SCRIPT_PATH}" ]]; then
            echo "Haciendo ejecutable el script de despliegue..."
            chmod +x "${SCRIPT_PATH}"
          fi
          echo "Script de despliegue validado: ${SCRIPT_PATH}"
          echo "Ejecutando despliegue para ${{ env.CLOUD_PROVIDER }} con ${DATA_PLATFORM}..."
          export DATA_PLATFORM="${DATA_PLATFORM}"
          export TERRAFORM_OUTPUTS="artifacts/terraform-outputs/terraform_outputs.json"
          bash "${SCRIPT_PATH}"
          echo "Despliegue para ${{ env.CLOUD_PROVIDER }} con ${DATA_PLATFORM} completado"

rollback-data-platform-config:
  needs: [configure-data-platform, detect-data-platform, deploy-infrastructure]
  runs-on: ubuntu-latest
  if: |
    always() && 
    needs.configure-data-platform.result == 'failure' &&
    env.ENABLE_ROLLBACK == 'true'
  steps:
    - name: Checkout code
      uses: actions/checkout@v4

    # AWS
    - name: Configure AWS Credentials
      if: env.CLOUD_PROVIDER == 'aws'
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.TF_VAR_region }}

    # Azure
    - name: Azure Login
      if: env.CLOUD_PROVIDER == 'azure'
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    # GCP
    - name: Authenticate to Google Cloud
      if: env.CLOUD_PROVIDER == 'gcp'
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ secrets.GCP_CREDENTIALS_JSON }}
        project_id: ${{ secrets.GCP_PROJECT_ID }}

    - name: Setup Google Cloud CLI
      if: env.CLOUD_PROVIDER == 'gcp'
      uses: google-github-actions/setup-gcloud@v2

    # Cleanup de configuraciones de plataforma de datos
    - name: Cleanup Data Platform Configurations
      run: |
        set -euo pipefail
        PLATFORM="${{ needs.detect-data-platform.outputs.platform }}"
        
        echo "ðŸ”„ Limpiando configuraciones de ${PLATFORM}..."
        
        case "${PLATFORM}" in
          databricks)
            echo "Instalando Databricks CLI..."
            pip install databricks-cli
            
            # Configurar
            databricks configure --token <<EOF
            ${{ secrets.DATABRICKS_HOST }}
            ${{ secrets.DATABRICKS_TOKEN }}
            EOF
            
            # Eliminar montajes y configuraciones
            echo "Eliminando scripts de configuraciÃ³n..."
            databricks workspace delete /Shared/setup/mount_s3 || echo "Script no existe"
            databricks workspace delete /Shared/setup/ -r || echo "Directorio no existe"
            
            echo "âœ… Configuraciones de Databricks eliminadas"
            ;;
            
          snowflake)
            echo "Instalando SnowSQL..."
            curl -O https://sfc-repo.snowflakecomputing.com/snowsql/bootstrap/1.2/linux_x86_64/snowsql-1.2.28-linux_x86_64.bash
            bash snowsql-1.2.28-linux_x86_64.bash
            
            # Crear config
            mkdir -p ~/.snowsql
            cat > ~/.snowsql/config <<EOF
            [connections.prod]
            accountname = ${{ secrets.SNOWFLAKE_ACCOUNT }}
            username = ${{ secrets.SNOWFLAKE_USER }}
            password = ${{ secrets.SNOWFLAKE_PASSWORD }}
            EOF
            
            # Eliminar integraciones y stages
            echo "Eliminando integraciones de almacenamiento..."
            snowsql -c prod -q "DROP STAGE IF EXISTS s3_stage;" || echo "Stage no existe"
            snowsql -c prod -q "DROP STORAGE INTEGRATION IF EXISTS s3_integration;" || echo "Integration no existe"
            
            echo "âœ… Configuraciones de Snowflake eliminadas"
            ;;
        esac



  rollback-on-failure:
    needs: [deploy-script-cloud-provider]
    runs-on: ubuntu-latest
    if: failure() && env.ENABLE_ROLLBACK == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS Credentials
        if: env.CLOUD_PROVIDER == 'aws'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.TF_VAR_region }}

      - name: Verify AWS Configuration
        if: env.CLOUD_PROVIDER == 'aws'
        run: |
          echo "Verificando credenciales AWS para rollback..."
          aws sts get-caller-identity
          
      - name: Azure Login
        if: env.CLOUD_PROVIDER == 'azure'
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Verify Azure Configuration
        if: env.CLOUD_PROVIDER == 'azure'
        run: |
          echo "Verificando credenciales Azure para rollback..."
          az account show
          
      - name: Authenticate to Google Cloud
        if: env.CLOUD_PROVIDER == 'gcp'
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS_JSON }}
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Setup Google Cloud CLI
        if: env.CLOUD_PROVIDER == 'gcp'
        uses: google-github-actions/setup-gcloud@v2

      - name: Verify GCP Configuration
        if: env.CLOUD_PROVIDER == 'gcp'
        run: |
          echo "Verificando credenciales GCP para rollback..."
          gcloud config list
          
      - name: Terraform Rollback
        run: |
          set -euo pipefail
          cd terraform/${{ env.CLOUD_PROVIDER }}
          echo "Iniciando rollback de infraestructura..."
          case "${{ env.CLOUD_PROVIDER }}" in
            aws)
              terraform init \
                -backend-config="bucket=${{ env.TF_BACKEND_BUCKET }}" \
                -backend-config="region=${{ env.TF_VAR_region }}" \
                -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_DYNAMODB_TABLE }}" \
                -input=false
              ;;
            azure)
              terraform init \
                -backend-config="storage_account_name=${{ secrets.TF_BACKEND_STORAGE_ACCOUNT }}" \
                -backend-config="resource_group_name=${{ secrets.TF_BACKEND_RESOURCE_GROUP }}" \
                -backend-config="container_name=${{ secrets.TF_BACKEND_CONTAINER_NAME }}" \
                -input=false
              ;;
            gcp)
              terraform init \
                -backend-config="bucket=${{ env.TF_BACKEND_BUCKET }}" \
                -input=false
              ;;
          esac
          echo "Plan de destrucciÃ³n:"
          terraform plan -destroy
          echo "Destruyendo recursos..."
          terraform destroy -auto-approve
          echo "Rollback completado exitosamente"

      - name: Notify Rollback
        if: always()
        run: |
          echo "Rollback ejecutado debido a fallas en el despliegue"
          echo "Proveedor: ${{ env.CLOUD_PROVIDER }}"
          echo "RegiÃ³n: ${{ env.TF_VAR_region }}"

  maintenance-cleanup:
    needs: deploy-script-cloud-provider
    runs-on: ubuntu-latest
    steps:
      # Paso 14: Cleanup
      - name: Cleanup Temporary Files
        if: always()
        run: |
          set +e 
          echo "Limpiando archivos temporales..."
          find . -type f -name "*.tmp" -delete
          rm -rf /tmp/* || true
          if [[ -f terraform/${{ env.CLOUD_PROVIDER }}/tfplan ]]; then
            rm -f terraform/${{ env.CLOUD_PROVIDER }}/tfplan
          fi
          echo "Limpieza completada"